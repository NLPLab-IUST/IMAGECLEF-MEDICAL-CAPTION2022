# -*- coding: utf-8 -*-
"""caption-generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Qn-PFxQ80kY89CdTh9_J0VS8kKLEllv

# Getting Files

imports
"""
 
import os
import csv
import h5py
import math
import cv2
import spacy, numpy as np
import glob
import matplotlib.pyplot as plt
import tensorflow as tf
from scipy.spatial import distance
import pandas as pd
import sys, argparse, string
import nltk
from nltk.translate.bleu_score import SmoothingFunction
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer

nltk.download("punkt")
nltk.download("stopwords")

path = "/home/MohammadMahdiJavid/mvqa/mvqa/IMAGECLEF/"
train_path = path + "IMAGECLEF_dataset/train/train"
valid_path = path + "IMAGECLEF_dataset/valid/valid/valid"
test_path = path + "IMAGECLEF_dataset/test/test/test"

"""create necessary folders"""
ysmn_path = path + "yasaman-caption"
os.makedirs(ysmn_path + "/results", exist_ok=True)
os.makedirs(ysmn_path + "/weights", exist_ok=True)


resnet_weights_path = ysmn_path + "/resnet50_weights_tf_dim_ordering_tf_kernels.h5"
weights_path = ysmn_path + "/weights/final_weights10epochs.h5"

train_limit = 83275
valid_limit = 7645
test_limit = 7601
dataset_batchsize = 64

"""# Preprocessing

Read and sample ***train_limit*** images from train set"""

train_df = (
    pd.read_csv(
        ysmn_path + "/ImageCLEFmedCaption_2022_caption_train_prepro.csv",
        delimiter="\t",
        index_col="ID",
    )
    .sample(train_limit)
    .sort_values(by=["ID"])
)
# train_df_limited = train_df.sample(train_limit).sort_values(by=['ID'])
print(f"2- picked {len(train_df)} train images")

"""Read and sample ***valid_limit*** images from valid set"""

valid_df = (
    pd.read_csv(
        ysmn_path + "/ImageCLEFmedCaption_2022_caption_valid_prepro.csv",
        delimiter="\t",
        index_col="ID",
    )
    .sample(valid_limit)
    .sort_values(by=["ID"])
)
# valid_df_limited = valid_df.sample(valid_limit).sort_values(by=['ID'])
print(f"3- picked {len(valid_df)} valid images")

"""## Preprocessing Images"""


def load_images_from_list(pixel_x, pixel_y, img_names, folder_path, apply_clahe=True):
    """normalize and loads image in array format"""
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    images_list = []

    for img in img_names:
        img_path = folder_path + "/" + img + ".jpg"
        image = cv2.imread(img_path, 0)
        if apply_clahe:
            image = clahe.apply(image)  # apply CLAHE
        # image = cv2.equalizeHist(image) #apply Histograms Equalization
        image = cv2.resize(
            image, (pixel_y, pixel_x), interpolation=cv2.INTER_AREA
        )  # resize image
        images_list.append(image)

    images_list_np = np.array(images_list)
    return images_list_np


train_img_names = train_df.index.values.tolist()
valid_img_names = valid_df.index.values.tolist()

vocab_stat = pd.read_csv(
    ysmn_path + "/caption_tokens_occurences.csv", index_col="token"
)
vocab_size = len(vocab_stat)

# Use a multilabelbinarizer to transform the concepts into a binary format for training
from sklearn.preprocessing import MultiLabelBinarizer

vocab_words = vocab_stat.index.values.tolist()
mlb = MultiLabelBinarizer(classes=vocab_words)
mlb.fit([vocab_words])


class My_Custom_Generator(tf.keras.utils.Sequence):
    def __init__(self, image_filenames, batch_size, folder_path, concepts_df):
        self.image_filenames = image_filenames
        self.batch_size = batch_size
        self.folder_path = folder_path
        self.concepts_df = concepts_df

    def __len__(self):
        return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(int)

    def __getitem__(self, idx):
        batch_names = self.image_filenames[
            idx * self.batch_size : (idx + 1) * self.batch_size
        ]
        batch_x = load_images_from_list(224, 224, batch_names, self.folder_path)
        batch_x = np.expand_dims(batch_x, axis=-1)
        batch_x = tf.image.grayscale_to_rgb(tf.convert_to_tensor(batch_x), name=None)
        batch_y = self.get_labels(batch_names)
        return batch_x, batch_y

    def get_labels(self, img_names):
        if self.concepts_df is None:
            return np.zeros((len(img_names), len(mlb.classes_)))
        return mlb.transform(
            self.concepts_df.loc[img_names]["caption"]
            .apply(lambda x: x.split(";"))
            .values
        )


gen_train = My_Custom_Generator(
    train_img_names, dataset_batchsize, folder_path=train_path, concepts_df=train_df
)
gen_valid = My_Custom_Generator(
    valid_img_names, dataset_batchsize, folder_path=valid_path, concepts_df=valid_df
)
print("4- created train and valid data generators")

"""# extracting image features

## Setup
"""

resnet50 = tf.keras.applications.ResNet50(
    include_top=True,
    weights=resnet_weights_path,
)

x = resnet50.layers[-2].output  # remove last layer
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.PReLU()(x)
x = tf.keras.layers.Dense(len(mlb.classes_), activation="sigmoid")(x)  # add layer
fine_tuned_resnet50 = tf.keras.models.Model(inputs=resnet50.input, outputs=x)
# fine_tuned_resnet50.load_weights(weights_path)


def get_bleu(gt_pairs, candidate_pairs):
    # calculate bleu w/o ordering
    min_words = sys.maxsize
    max_words = 0
    max_sent = 0
    total_words = 0
    words_distrib = {}
    current_score = 0
    max_score = len(gt_pairs)

    for i in range(len(candidate_pairs)):
        gt_words = gt_pairs[i]
        total_words += len(gt_words)
        candidate_words = candidate_pairs[i]

        try:
            # If both the GT and candidate are empty, assign a score of 1 for this caption
            if len(gt_words) == 0 and len(candidate_words) == 0:
                bleu_score = 1
            # Calculate the BLEU score
            else:
                bleu_score = nltk.translate.bleu_score.sentence_bleu(
                    [gt_words],
                    candidate_words,
                    smoothing_function=SmoothingFunction().method0,
                )
        # Handle problematic cases where BLEU score calculation is impossible
        except ZeroDivisionError:
            print("Problem with ", gt_words, candidate_words)
            # Increase calculated score
        # print(i, " score ", bleu_score)
        current_score += bleu_score
        nb_words = str(len(gt_words))
        if nb_words not in words_distrib:
            words_distrib[nb_words] = 1
        else:
            words_distrib[nb_words] += 1

        # Corpus stats
        if len(gt_words) > max_words:
            max_words = len(gt_words)

        if len(gt_words) < min_words:
            min_words = len(gt_words)

    #         if len(gt_sentences) > max_sent:
    #             max_sent = len(gt_sentences)

    # Progress display
    # i += 1
    # if i % 200 == 0:
    #     print(i, '/', len(gt_pairs), ' captions processed...')

    # print('Corpus statistics\n********************************')
    #         print('Number of words distribution')
    #         print_dict_sorted_num(words_distrib)
    # print('Least words in caption :', min_words)
    # print('Most words in caption :', max_words)
    # print('Average words in caption :', total_words / len(gt_pairs))
    # print('Most sentences in caption :', max_sent)

    # Print evaluation result
    # print('Final result')
    # print('Obtained score :', current_score, '/', max_score)
    # print('Mean score over all captions :', current_score / max_score)
    return current_score / max_score


initial_learning_rate = 5e-4
fine_tuned_resnet50.compile(
    loss="binary_crossentropy",
    optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate),
    metrics=["acc"],
)

"""## Training"""


class Test_Custom_Generator(tf.keras.utils.Sequence):
    def __init__(self, image_filenames, batch_size, folder_path):
        self.image_filenames = image_filenames
        self.batch_size = batch_size
        self.folder_path = folder_path

    def __len__(self):
        return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(int)

    def __getitem__(self, idx):
        batch_names = self.image_filenames[
            idx * self.batch_size : (idx + 1) * self.batch_size
        ]
        batch_x = load_images_from_list(224, 224, batch_names, self.folder_path)
        batch_x = np.expand_dims(batch_x, axis=-1)
        batch_x = tf.image.grayscale_to_rgb(tf.convert_to_tensor(batch_x), name=None)
        return batch_x


import glob

test_img_names = [
    os.path.splitext(os.path.basename(x))[0] for x in glob.glob(test_path + "/*.jpg")
][:test_limit]
gen_test = Test_Custom_Generator(
    test_img_names, dataset_batchsize, folder_path=test_path
)


def predict_and_save(valid_pred, test_pred, epoch):
    Ns = [20, 21, 22, 23, 24, 25, 26, 27]
    bleus_normal = []
    bleus_sorted = []
    for N in Ns:
        # turn to words
        # print("==================\nN: ", N)
        valid_words_idx = np.argsort(-valid_pred, axis=1)[:, :N]
        valid_words = np.take(mlb.classes_, valid_words_idx)
        # print("normal: ")
        bleu = get_bleu(valid_df["caption"].apply(lambda x: x.split(";")), valid_words)
        bleus_normal.append(bleu)
        valid_captions_sorted = []
        for v in valid_words:
            v_stats = [int(vocab_stat.loc[v_token]["occurences"][0]) for v_token in v]
            valid_captions_sorted.append(v[np.argsort(v_stats)])
        # print("example: ", valid_captions_sorted[0])
        # print("sorted: ")

        bleu = get_bleu(
            valid_df["caption"].apply(lambda x: x.split(";")), valid_captions_sorted
        )
        bleus_sorted.append(bleu)

    pd.DataFrame({"N": Ns, "normal bleu": bleus_normal, "sorted": bleus_sorted}).to_csv(
        ysmn_path + f"/results/valid_bleus_e{epoch}_.csv", index=False
    )

    print("generating test: ")
    for N in Ns:
        # turn to words
        # print("==================\nN: ", N)
        valid_words_idx = np.argsort(-(test_pred), axis=1)[:, :N]
        valid_words = np.take(mlb.classes_, valid_words_idx)
        valid_captions_sorted = []
        for v in valid_words:
            v_stats = [int(vocab_stat.loc[v_token]["occurences"][0]) for v_token in v]
            valid_captions_sorted.append(v[np.argsort(v_stats)])
            # print(v[np.argsort(v_stats)])
        # print("example: ", valid_captions_sorted[0])
        pd.DataFrame(
            data={
                "ID": test_img_names,
                "caption": [" ".join(tokens) for tokens in valid_captions_sorted],
            }
        ).to_csv(
            ysmn_path + "/results" + f"/test_res_N_{N}_e{epoch}_sorted.csv",
            index=False,
            sep="\t",
        )
        pd.DataFrame(
            data={
                "ID": test_img_names,
                "caption": [" ".join(tokens) for tokens in valid_captions_sorted],
            }
        ).to_csv(
            ysmn_path + "/results" + f"/test_res_N_{N}_e{epoch}_normal.csv",
            index=False,
            sep="\t",
        )


class SaveResultsCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if (epoch + 1) % 5 != 0 and (epoch + 1) != 1:
            return
        test_pred = self.model.predict(gen_test)
        valid_pred = self.model.predict(gen_valid)

        predict_and_save(valid_pred, test_pred, epoch + 1)

        self.model.save_weights(
            ysmn_path + f"/weights/e{epoch+1}_weights.h5", overwrite=True
        )


history = fine_tuned_resnet50.fit(
    x=gen_train,
    epochs=50,
    validation_data=gen_valid,
    verbose=1,
    callbacks=[SaveResultsCallback()],
)
